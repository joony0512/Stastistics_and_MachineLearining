# 4.KNN(k-Nearest Neighbors)

## 0.개요

- lazy learner에 비해 perfomance는 좋음
- 고차원 공간이 되면 계산이 늦어진다.
- Curse of dimension
    - 1 차원 1/2 =50%
    - 2 차원 0.71**2 = 50%
    - 3 차원 0.79**3 = 50%
    - 차원이 높아질수록 성능이 저하된다.
    - p=2이면 유클리디안 거리와 같음
        - p는 hyper parameter → 다해보고 제일 좋은 거 찾아라
- k 도 하이퍼 파라미터임 → 다해보고 제일 좋은 거 알아서 찾아라
    - k가 작을수록 정교하게 나누어진다.
    - lazy learner( input이 들어올때마다 새로학습)이기때문에 overfitting문제는 딱히 걱정 하지 않아도 된다.

## 1.KNN의 적용 : 6p

- 회귀 분석에도 사용 가능하다
    - k개의 관측치의 y-bar를 계산하여 적합치로 사용
    - 해석을 위한 회귀가 아니고, 예측을 위해 만든 것이기 때문에, 사용해도 괜찮고 성능도 괜찮다.
- 10p : train_test_split : 무조건 트레인 데이터와 테스트 데이터로 나누어야 한다.
    - 30%를 테스트로 뽑음
    - 데이터를 순서대로 잘 정리한 상태면 안된다 → random_state사용
    - stratify = y(yes라는 뜻) : 품종별로 골고루 뽑기
        - 내가 갖고 있는 자료에 분류의 정보가 있다면 분류별로 골고루 뽑는다.
- 12p : knn의 적용

## 2.혼동행렬 (Confusion Matrix) - 15 p~

### 성능측정 : 불완전한 알고리즘이 주어졌을 때 정답이 나올 가능성은 얼마나 될까? - 오차의 이해가 중요

- 정밀도
- 재현률
- 정확도
- 16 p
    - Actual Value : 실제 데이터
    - Prediction : 예측 데이터
    - 혼동행렬 값이름 : 잘 분류 됐는가 + 예측값
    
- 17 p
    - 정확도 : 예측이 어떻게 잘못된 것인지 알려주지는 않지만, 얼마나 정확한 결과를 얻을 수 있는지에 대한 직관적인 정보를 준다.
        - 오차의 정보가 빠져있다. 맹신하면 안되는 이유.
- 18 p
    - 정밀도(Precision) :  양성 예측의 몇%가 정확했는지에 대하여 알려줌
        - 실제양성이며 예측양성(TP) / 실제양성
- 19 p
    - 재현률(Recall) : 올바르게 예측한 양성 표본의 비율
        - 실제양성이며 예측양성(TP) / 실제양성
        - type 1 error(FP) 보다 type 2 error(FN)이 더 위험하기 때문에 type1을 고정시키고 type2를 minimize해야한다
            - 코로나인사람을 아니라고 예측하는 것이 더 위험하기 때문이다.
- 20 p
    - 정밀도와 재현율의 trade-off
        - 거짓양성(FP)을 줄이면 거짓음성(FN)은 증가한다.
- 22 p
    - f1점수 : 정밀도와 재현율의 조화평균
- 23 p
    - 특이도 :  실제 음성표본을 음성이라고 분류한 비율
    

## 3.커널분포함수 추정 (Kernel density estimation)

-
